<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Learning - Clustering</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            background-color: #ffffff;
            color: #333;
            font-family: 'Arial', sans-serif;
        }

        h1,
        h2,
        h3 {
            color: #007BFF;
        }

        .content-section {
            margin-bottom: 40px;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        .highlight {
            background-color: #f1f8ff;
            border-left: 4px solid #007BFF;
            padding: 10px 15px;
            margin: 15px 0;
        }

        .algorithm-list {
            list-style-type: none;
            padding-left: 0;
        }

        .algorithm-list li {
            margin: 10px 0;
            padding: 10px;
            background: #f9f9f9;
            border-radius: 5px;
        }

        .evaluation-metrics,
        .visualization {
            margin-top: 20px;
        }

        .metric-item {
            margin: 5px 0;
            padding: 10px;
            border: 1px solid #007BFF;
            border-radius: 5px;
            background: #e9f7ff;
        }

        .footer {
            font-size: 0.9em;
            color: #777;
            text-align: center;
            margin-top: 20px;
        }
    </style>
</head>

<body>

    <div class="container mt-5">
        <div class="content-section">
            <h1>Week 5: Unsupervised Learning - Clustering</h1>

            <h2>K-Means Clustering and Hierarchical Clustering</h2>
            <p>Clustering is a technique used to group similar data points together. K-means clustering and hierarchical
                clustering are two popular algorithms for clustering.</p>

            <h3>1. K-Means Clustering</h3>
            <div class="highlight">
                <strong>Algorithm:</strong>
                <ol class="algorithm-list">
                    <li>Initialize K random points as cluster centroids.</li>
                    <li>Assign each data point to the nearest centroid.</li>
                    <li>Recompute the centroids as the mean of the assigned data points.</li>
                    <li>Repeat steps 2 and 3 until convergence.</li>
                </ol>
            </div>
            <div class="highlight">
                <strong>Advantages:</strong>
                <ul>
                    <li>Simple and efficient for large datasets.</li>
                    <li>Can be easily parallelized.</li>
                </ul>
            </div>
            <div class="highlight">
                <strong>Disadvantages:</strong>
                <ul>
                    <li>Requires specifying the number of clusters beforehand.</li>
                    <li>Sensitive to the choice of initial centroids.</li>
                    <li>May not work well with non-spherical clusters.</li>
                </ul>
            </div>

            <h3>2. Hierarchical Clustering</h3>
            <div class="highlight">
                <strong>Algorithm:</strong>
                <ol class="algorithm-list">
                    <li>Start with each data point as a separate cluster.</li>
                    <li>Merge the two closest clusters.</li>
                    <li>Repeat step 2 until there is only one cluster.</li>
                </ol>
            </div>
            <div class="highlight">
                <strong>Types:</strong>
                <ul>
                    <li><strong>Agglomerative clustering:</strong> Starts with individual clusters and merges them.</li>
                    <li><strong>Divisive clustering:</strong> Starts with one large cluster and divides it into smaller
                        clusters.</li>
                </ul>
            </div>
            <div class="highlight">
                <strong>Distance metrics:</strong> Euclidean distance, Manhattan distance, cosine similarity, etc.
                <br>
                <strong>Linkage methods:</strong> Single-linkage, complete-linkage, average-linkage, centroid-linkage.
            </div>

            <h3>Choosing the Right Algorithm</h3>
            <p>The choice of clustering algorithm depends on the characteristics of your data and the specific goals of
                your analysis. Consider the following factors:</p>
            <ul>
                <li><strong>Number of clusters:</strong> If you have a known number of clusters, K-means clustering may
                    be appropriate.</li>
                <li><strong>Cluster shape:</strong> Hierarchical clustering can handle clusters of different shapes.
                </li>
                <li><strong>Computational resources:</strong> K-means clustering is generally more efficient than
                    hierarchical clustering for large datasets.</li>
            </ul>

            <h3>Evaluation Metrics</h3>
            <div class="evaluation-metrics">
                <div class="metric-item"><strong>Silhouette coefficient:</strong> Measures how similar a data point is
                    to its own cluster compared to other clusters.</div>
                <div class="metric-item"><strong>Calinski-Harabasz index:</strong> Measures the ratio of between-cluster
                    variance to within-cluster variance.</div>
                <div class="metric-item"><strong>Davies-Bouldin index:</strong> Measures the average similarity between
                    each cluster and its nearest cluster.</div>
            </div>

            <h3>Visualization</h3>
            <div class="visualization">
                <ul>
                    <li><strong>Dendrogram:</strong> A tree-like diagram that represents the hierarchical structure of
                        clusters.</li>
                    <li><strong>Scatter plots:</strong> Visualize the clusters in two-dimensional space.</li>
                </ul>
            </div>

            <h3>Conclusion</h3>
            <p>By understanding K-means clustering and hierarchical clustering, you can effectively group data points
                and discover meaningful patterns within your dataset. Mastering these techniques opens the door to
                advanced analytics, providing valuable insights into your data.</p>
        </div>
    </div>

    <div class="footer">
        <p>Â© 2025 Unsupervised Learning Course</p>
    </div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>

</html>