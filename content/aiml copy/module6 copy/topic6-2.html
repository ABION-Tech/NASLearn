<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 6: Advanced Supervised Learning - Support Vector Machines (SVMs)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            background-color: white;
            color: black;
            font-family: Arial, sans-serif;
        }

        h1,
        h2,
        h3 {
            color: #007bff;
            /* Bootstrap primary color */
        }

        h1 {
            margin-top: 30px;
            margin-bottom: 20px;
        }

        h2 {
            margin-top: 20px;
            margin-bottom: 15px;
        }

        h3 {
            margin-top: 15px;
            margin-bottom: 10px;
        }

        p {
            line-height: 1.6;
        }

        .section {
            margin-bottom: 40px;
        }

        .code {
            background-color: #f8f9fa;
            border-left: 3px solid #007bff;
            padding: 10px;
            overflow-x: auto;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Week 6: Advanced Supervised Learning - Support Vector Machines (SVMs)</h1>

        <div class="section">
            <h2>Support Vector Machines (SVMs)</h2>
            <p>Support Vector Machines (SVMs) are a powerful class of supervised learning algorithms primarily used for
                classification tasks, especially when the dataset contains high-dimensional feature spaces. The
                fundamental objective of SVM is to determine the hyperplane that best separates the different classes in
                the dataset while maximizing the margin between them.</p>
        </div>

        <div class="section">
            <h2>1. Linear SVM</h2>
            <p>The linear SVM seeks to establish a hyperplane, which is a linear decision boundary, that divides the
                classes within the dataset. Key concepts include:</p>
            <ul>
                <li><strong>Hyperplane:</strong> A linear decision boundary that separates the data into two classes.
                </li>
                <li><strong>Margin:</strong> The distance between the hyperplane and the closest data points of each
                    class.</li>
                <li><strong>Optimization:</strong> SVM optimizes the position of the hyperplane to maximize the margin.
                </li>
            </ul>
            <p>The mathematical formulation for a linear SVM can be expressed as:</p>
            <div class="code">
                maximize: γ <br>
                subject to: y<sub>i</sub>(w⋅x<sub>i</sub> + b) ≥ 1 for all i
            </div>
            <p>Where:</p>
            <ul>
                <li><code>y<sub>i</sub></code>: The class label of the i<sup>th</sup> data point.</li>
                <li><code>x<sub>i</sub></code>: The i<sup>th</sup> data point.</li>
                <li><code>w</code>: The normal vector to the hyperplane.</li>
                <li><code>b</code>: The bias term.</li>
                <li><code>γ</code>: The margin.</li>
            </ul>
        </div>

        <div class="section">
            <h2>2. Kernel Trick</h2>
            <p>Many real-world datasets are not linearly separable. The kernel trick is a technique used in SVMs to
                transform data into a higher-dimensional space, making it easier to separate the classes linearly.</p>
            <ul>
                <li><strong>Common Kernels:</strong>
                    <ul>
                        <li><strong>Linear Kernel:</strong> Used for linearly separable data.</li>
                        <li><strong>Polynomial Kernel:</strong> Captures non-linear relationships.</li>
                        <li><strong>Radial Basis Function (RBF) Kernel:</strong> A commonly used kernel for non-linear
                            relationships.</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="section">
            <h2>3. Soft Margin SVM</h2>
            <p>To improve generalization and handle outliers, SVM employs a soft margin approach, which allows for some
                misclassifications. The regularization parameter (C) controls the balance between maximizing the margin
                and minimizing misclassifications:</p>
            <ul>
                <li><strong>Regularization Parameter (C):</strong> A hyperparameter that dictates how much we want to
                    avoid misclassifying each training example.</li>
            </ul>
        </div>

        <div class="section">
            <h2>4. Evaluation Metrics</h2>
            <p>To assess the performance of SVM models, several metrics can be utilized:</p>
            <ul>
                <li><strong>Accuracy:</strong> The proportion of correct predictions made by the model.</li>
                <li><strong>Precision:</strong> The ratio of true positive predictions to the total predicted positives.
                </li>
                <li><strong>Recall:</strong> The ratio of true positive predictions to the actual positives.</li>
                <li><strong>F1-score:</strong> The harmonic mean of precision and recall.</li>
                <li><strong>ROC Curve:</strong> A graphical representation of the true positive rate versus the false
                    positive rate.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Applications of SVM</h2>
            <p>SVMs are versatile algorithms with applications across various domains, including:</p>
            <ul>
                <li><strong>Text Classification:</strong> Used to categorize text documents (e.g., spam detection,
                    sentiment analysis).</li>
                <li><strong>Image Classification:</strong> Employed to recognize and classify objects in images.</li>
                <li><strong>Bioinformatics:</strong> Used to predict protein structure and function.</li>
                <li><strong>Anomaly Detection:</strong> Effective in identifying unusual data points within datasets.
                </li>
            </ul>
        </div>

        <div class="section">
            <h2>Advantages and Disadvantages</h2>
            <p>While SVMs are powerful, they come with both advantages and disadvantages:</p>
            <ul>
                <li><strong>Advantages:</strong>
                    <ul>
                        <li>Effective for high-dimensional datasets.</li>
                        <li>Robust against outliers due to the margin maximization approach.</li>
                        <li>Can model non-linear relationships using kernel functions.</li>
                    </ul>
                </li>
                <li><strong>Disadvantages:</strong>
                    <ul>
                        <li>Computationally intensive, especially with large datasets.</li>
                        <li>Sensitive to the choice of kernel and regularization parameter.</li>
                    </ul>
                </li>
            </ul>
            <p>By understanding the principles of SVM and their applications, you can leverage this powerful algorithm
                effectively across various classification problems.</p>
        </div>

        <div class="section">
            <h2>Model Tuning: Grid Search and Random Search</h2>
            <p>Model tuning is a critical step in machine learning, involving the adjustment of hyperparameters to
                enhance the model's performance. Hyperparameters are the settings configured before the model training
                process.</p>

            <h3>1. Grid Search</h3>
            <p>Grid search is a systematic method for exploring a specified grid of hyperparameter values:</p>
            <ul>
                <li><strong>Process:</strong>
                    <ol>
                        <li>Define a grid of hyperparameter values.</li>
                        <li>Train a model for each combination of hyperparameter values.</li>
                        <li>Evaluate the performance of each model using a validation set.</li>
                        <li>Select the model with the best performance.</li>
                    </ol>
                </li>
                <li><strong>Advantages:</strong> Guarantees the discovery of the best hyperparameters within the defined
                    grid.</li>
                <li><strong>Disadvantages:</strong> Can be computationally expensive when the grid is large.</li>
            </ul>

            <h3>2. Random Search</h3>
            <p>Random search randomly samples hyperparameter values from predefined distributions:</p>
            <ul>
                <li><strong>Process:</strong>
                    <ol>
                        <li>Define a distribution for each hyperparameter.</li>
                        <li>Randomly sample hyperparameter values from the distributions.</li>
                        <li>Train a model for each combination of sampled hyperparameters.</li>
                        <li>Evaluate the performance of each model using a validation set.</li>
                        <li>Repeat the previous steps for a specified number of iterations.</li>
                    </ol>
                </li>
                <li><strong>Advantages:</strong> More efficient than grid search for large hyperparameter spaces and can
                    explore a broader range of values.</li>
                <li><strong>Disadvantages:</strong> Does not guarantee finding the optimal hyperparameter values.</li>
            </ul>

            <h3>Choosing the Right Technique</h3>
            <p>The choice between grid search and random search depends on the hyperparameter space:</p>
            <ul>
                <li><strong>Grid search:</strong> Best suited for small hyperparameter spaces.</li>
                <li><strong>Random search:</strong> Ideal for larger hyperparameter spaces or when computational
                    resources are limited.</li>
            </ul>

            <h3>Additional Considerations</h3>
            <ul>
                <li><strong>Nested Cross-Validation:</strong> Use nested cross-validation to evaluate model performance
                    on unseen data while tuning hyperparameters.</li>
                <li><strong>Bayesian Optimization:</strong> A more efficient method using Bayesian probability to
                    explore the hyperparameter space.</li>
                <li><strong>Hyperparameter Importance:</strong> Identify the hyperparameters that significantly affect
                    the model's performance.</li>
            </ul>

            <h3>Practical Tips</h3>
            <ul>
                <li>Start with a small grid or distribution.</li>
                <li>Utilize a validation set to assess performance.</li>
                <li>Consider the computational cost when selecting hyperparameter values.</li>
                <li>Experiment with different tuning techniques.</li>
            </ul>

            <p>By effectively tuning the hyperparameters of your machine learning models, you can improve their
                performance and achieve better results.</p>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
</body>

</html>