<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 4: Unsupervised Learning - Clustering</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            background-color: white;
            color: black;
            font-family: Arial, sans-serif;
        }

        h1,
        h2,
        h3 {
            color: #007bff;
            /* Bootstrap primary color */
        }

        .card {
            border: none;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            margin-bottom: 20px;
        }

        .card-header {
            background-color: #007bff;
            color: white;
        }

        .highlight {
            background-color: #f0f8ff;
            /* Light blue background for emphasis */
            border-left: 5px solid #007bff;
            padding: 10px;
        }

        .steps-list {
            list-style-type: none;
            padding-left: 0;
        }

        .steps-list li {
            position: relative;
            padding-left: 20px;
        }

        .steps-list li:before {
            content: 'â€¢';
            position: absolute;
            left: 0;
            color: #007bff;
            /* Blue bullet point */
        }

        .footer {
            padding: 20px 0;
            text-align: center;
        }
    </style>
</head>

<body>

    <div class="container mt-5">

        <h1 class="text-center mb-4">Week 4: Unsupervised Learning - Clustering</h1>

        <div class="card">
            <div class="card-header">Dimensionality Reduction: Principal Component Analysis (PCA)</div>
            <div class="card-body">
                <p>
                    Dimensionality reduction is a powerful technique used to reduce the number of features in a dataset
                    while preserving its most essential information. This process is beneficial for several reasons:
                </p>
                <div class="highlight">
                    <ul>
                        <li><strong>Improving computational efficiency:</strong> By reducing the dimensionality of data,
                            machine learning algorithms can run faster and more efficiently.</li>
                        <li><strong>Visualizing data:</strong> High-dimensional data can be challenging to visualize;
                            dimensionality reduction allows for clearer insights in lower-dimensional spaces.</li>
                        <li><strong>Reducing noise:</strong> This technique can help eliminate noise and irrelevant
                            features from the dataset, leading to improved model performance.</li>
                    </ul>
                </div>

                <h2>PCA Explained</h2>
                <p>
                    Principal Component Analysis (PCA) is one of the most widely used dimensionality reduction
                    techniques. It identifies the principal components of a dataset, which are linear combinations of
                    the original features that capture the most variance in the data.
                </p>

                <h3>Steps for Performing PCA</h3>
                <ol class="steps-list">
                    <li><strong>Standardize the data:</strong> Ensure all features have a mean of 0 and a standard
                        deviation of 1.</li>
                    <li><strong>Calculate the covariance matrix:</strong> Compute the covariance matrix of the
                        standardized data to understand the relationships between the variables.</li>
                    <li><strong>Compute eigenvectors and eigenvalues:</strong> Identify the eigenvectors (directions of
                        variance) and eigenvalues (magnitude of variance) of the covariance matrix.</li>
                    <li><strong>Select principal components:</strong> Choose eigenvectors corresponding to the largest
                        eigenvalues. These represent the directions with maximum variance.</li>
                    <li><strong>Project the data:</strong> Transform the original data into a lower-dimensional space
                        using the selected eigenvectors.</li>
                </ol>

                <h3>Choosing the Number of Principal Components</h3>
                <p>
                    Determining how many principal components to retain is crucial for effective dimensionality
                    reduction. Several methods can help in making this decision:
                </p>
                <ul>
                    <li><strong>Explained variance ratio:</strong> Analyze the proportion of variance each principal
                        component captures.</li>
                    <li><strong>Scree plot:</strong> Plot eigenvalues in descending order and identify the elbow point
                        where the slope of the curve decreases significantly.</li>
                    <li><strong>Cumulative explained variance:</strong> Calculate the cumulative variance explained by
                        the first k principal components to assess the total variance captured.</li>
                </ul>

                <h3>Applications of PCA</h3>
                <p>
                    PCA has a wide range of applications across various fields, including:
                </p>
                <ul>
                    <li><strong>Data visualization:</strong> Transforming high-dimensional data for better visual
                        insights.</li>
                    <li><strong>Feature engineering:</strong> Creating new features as linear combinations of the
                        original ones to enhance model performance.</li>
                    <li><strong>Noise reduction:</strong> Filtering out noise and irrelevant features from the data.
                    </li>
                    <li><strong>Image compression:</strong> Compressing images by reducing their dimensionality while
                        maintaining essential information.</li>
                    <li><strong>Natural language processing:</strong> Reducing dimensionality in word embeddings for
                        better computational efficiency.</li>
                </ul>

                <p>
                    By understanding the principles of PCA, you can effectively reduce the dimensionality of your data,
                    leading to more efficient and interpretable machine learning models.
                </p>
            </div>
        </div>

    </div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.0.11/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>

</html>