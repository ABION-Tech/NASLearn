<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            background-color: #ffffff;
            color: #333333;
        }

        h1,
        h2,
        h3 {
            color: #007bff;
        }

        h2 {
            margin-top: 30px;
            margin-bottom: 20px;
        }

        h3 {
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .metric {
            background-color: #f8f9fa;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin-bottom: 20px;
            border-radius: 5px;
        }

        .formula {
            background-color: #e9ecef;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
        }

        .note {
            font-size: 0.9rem;
            color: #6c757d;
        }

        .mt-5 {
            margin-top: 3rem !important;
        }
    </style>
    <title>Week 4: Supervised Learning - Classification</title>
</head>

<body>
    <div class="container mt-5">
        <h1>Week 4: Supervised Learning - Classification</h1>

        <h2>Model Evaluation: Accuracy, Precision, Recall, F1-Score, ROC Curve, and AUC</h2>
        <p>Evaluating the performance of a classification model is crucial to ensure its accuracy and reliability.
            Several metrics can be used to assess the model's effectiveness.</p>

        <div class="metric">
            <h3>1. Accuracy</h3>
            <p><strong>Definition:</strong> The proportion of correct predictions out of the total number of
                predictions.</p>
            <p class="formula">Accuracy = (TP + TN) / (TP + TN + FP + FN)</p>
            <p><strong>Where:</strong></p>
            <ul>
                <li><strong>TP:</strong> True positives</li>
                <li><strong>TN:</strong> True negatives</li>
                <li><strong>FP:</strong> False positives</li>
                <li><strong>FN:</strong> False negatives</li>
            </ul>
            <p><strong>Limitations:</strong> Can be misleading in imbalanced datasets.</p>
        </div>

        <div class="metric">
            <h3>2. Precision</h3>
            <p><strong>Definition:</strong> The proportion of positive predictions that were actually correct.</p>
            <p class="formula">Precision = TP / (TP + FP)</p>
            <p><strong>Interpretation:</strong> Measures the model's ability to avoid false positives.</p>
        </div>

        <div class="metric">
            <h3>3. Recall</h3>
            <p><strong>Definition:</strong> The proportion of actual positive instances that were correctly predicted.
            </p>
            <p class="formula">Recall = TP / (TP + FN)</p>
            <p><strong>Interpretation:</strong> Measures the model's ability to avoid false negatives.</p>
        </div>

        <div class="metric">
            <h3>4. F1-Score</h3>
            <p><strong>Definition:</strong> The harmonic mean of precision and recall.</p>
            <p class="formula">F1-score = 2 * (Precision * Recall) / (Precision + Recall)</p>
            <p><strong>Interpretation:</strong> Provides a balanced measure of precision and recall, useful when the
                class distribution is uneven.</p>
        </div>

        <div class="metric">
            <h3>5. ROC Curve (Receiver Operating Characteristic Curve)</h3>
            <p><strong>Definition:</strong> A plot that shows the true positive rate (TPR) against the false positive
                rate (FPR) for different classification thresholds.</p>
            <p><strong>AUC (Area Under the Curve):</strong> The area under the ROC curve, which quantifies the overall
                ability of the model to discriminate between classes.</p>
            <p><strong>Interpretation:</strong></p>
            <ul>
                <li>A higher AUC indicates better performance.</li>
                <li>An AUC of 1 represents perfect classification, while an AUC of 0.5 represents random guessing.</li>
            </ul>
        </div>

        <h2>Choosing the Right Metrics</h2>
        <p>The choice of evaluation metrics depends on the specific requirements of your problem. Consider the following
            factors:</p>
        <ul>
            <li><strong>Class imbalance:</strong> If the classes are imbalanced, precision, recall, and F1-score may be
                more informative than accuracy.</li>
            <li><strong>Cost of misclassification:</strong> If false positives or false negatives have different costs,
                you may want to prioritize precision or recall accordingly.</li>
            <li><strong>Trade-off between precision and recall:</strong> The choice between precision and recall depends
                on the specific application. For example, in a medical diagnosis problem, you might prioritize recall to
                avoid false negatives.</li>
        </ul>
        <p class="note">By understanding and using these evaluation metrics, you can effectively assess the performance
            of your classification models and make informed decisions about model selection and tuning.</p>
    </div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.2/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>

</html>