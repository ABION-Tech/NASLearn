<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 5: Unsupervised Learning - Clustering</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            background-color: white;
            color: black;
            font-family: Arial, sans-serif;
            line-height: 1.6;
        }

        h1,
        h2,
        h3 {
            color: #007bff;
            /* Blue accent color */
        }

        h1 {
            margin-top: 30px;
            font-size: 2.5rem;
        }

        h2 {
            margin-top: 20px;
            font-size: 2rem;
        }

        h3 {
            margin-top: 15px;
            font-size: 1.5rem;
        }

        p {
            margin: 10px 0;
        }

        ul {
            margin-left: 20px;
            list-style-type: disc;
        }

        .container {
            margin: 40px auto;
            max-width: 800px;
        }

        .card {
            margin-bottom: 20px;
            border: none;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .card-header {
            background-color: #007bff;
            /* Blue accent color */
            color: white;
            font-weight: bold;
        }

        .section {
            margin-bottom: 30px;
        }
    </style>
</head>

<body>

    <div class="module">
        <header class="module-header">
            <h1>Module 5: Neural Networks & Deep Learning</h1>
            <h2>Page 3: Training Neural Networks</h2>
        </header>

        <section class="content">
            <article class="introduction">
                <h2>How Do Neural Networks Learn?</h2>
                <p>Training a neural network involves adjusting its internal parameters (weights and biases) so that it
                    can make accurate predictions. This process requires multiple iterations of data processing, error
                    correction, and optimization.</p>
            </article>

            <article class="forward-propagation">
                <h2>1. Forward Propagation</h2>
                <p>During forward propagation, input data passes through the network layer by layer, getting transformed
                    by the neurons until it reaches the output layer.</p>
                <ul>
                    <li><strong>Inputs:</strong> Data is fed into the input layer.</li>
                    <li><strong>Weights & Biases:</strong> Each connection between neurons has a weight, and each neuron
                        has a bias.</li>
                    <li><strong>Activation Functions:</strong> Non-linear functions (e.g., ReLU, Sigmoid, Tanh)
                        determine neuron activation.</li>
                    <li><strong>Output:</strong> The final prediction of the model.</li>
                </ul>
            </article>

            <article class="loss-function">
                <h2>2. Loss Function: Measuring Errors</h2>
                <p>After making a prediction, the network needs to measure how far off it is from the actual answer.
                    This is done using a loss function.</p>
                <ul>
                    <li><strong>Common Loss Functions:</strong></li>
                    <ul>
                        <li><em>Mean Squared Error (MSE):</em> Used for regression tasks.</li>
                        <li><em>Cross-Entropy Loss:</em> Used for classification tasks.</li>
                        <li><em>Hinge Loss:</em> Used in SVM-based neural networks.</li>
                    </ul>
                    <li>The smaller the loss, the better the network’s prediction.</li>
                </ul>
            </article>

            <article class="backpropagation">
                <h2>3. Backpropagation: Adjusting Weights</h2>
                <p>Backpropagation is the method used to adjust the neural network's weights by minimizing the loss.</p>
                <ul>
                    <li><strong>Step 1:</strong> Compute the gradient of the loss function with respect to each weight.
                    </li>
                    <li><strong>Step 2:</strong> Use these gradients to update the weights to reduce the error.</li>
                    <li><strong>Step 3:</strong> Repeat the process for multiple training iterations.</li>
                </ul>
            </article>

            <article class="gradient-descent">
                <h2>4. Gradient Descent: Optimizing Learning</h2>
                <p>Gradient descent is the algorithm that helps backpropagation find the best weights efficiently.</p>
                <ul>
                    <li><strong>Types of Gradient Descent:</strong></li>
                    <ul>
                        <li><em>Batch Gradient Descent:</em> Updates weights after processing the entire dataset.</li>
                        <li><em>Stochastic Gradient Descent (SGD):</em> Updates weights after each training example.
                        </li>
                        <li><em>Mini-Batch Gradient Descent:</em> A mix of both, updating weights after a small batch of
                            data.</li>
                    </ul>
                    <li><strong>Challenges:</strong> Learning rate selection is crucial—too high causes instability, too
                        low slows training.</li>
                </ul>
            </article>

            <article class="optimization">
                <h2>5. Optimization Techniques</h2>
                <p>To improve training speed and accuracy, several advanced optimization algorithms are used.</p>
                <ul>
                    <li><strong>Adam Optimizer:</strong> Combines momentum and adaptive learning rates for faster
                        convergence.</li>
                    <li><strong>RMSprop:</strong> Reduces oscillations in weight updates.</li>
                    <li><strong>Momentum:</strong> Helps prevent getting stuck in local minima.</li>
                </ul>
            </article>

            <article class="overfitting">
                <h2>6. Overfitting & Underfitting</h2>
                <p>Neural networks can either memorize the training data (overfitting) or fail to learn patterns
                    effectively (underfitting).</p>
                <ul>
                    <li><strong>Overfitting:</strong> The model learns noise instead of actual patterns.</li>
                    <li><strong>Underfitting:</strong> The model is too simple and fails to capture complex patterns.
                    </li>
                    <li><strong>Solutions:</strong></li>
                    <ul>
                        <li>Use more training data.</li>
                        <li>Apply regularization (L1/L2).</li>
                        <li>Introduce dropout layers to randomly deactivate neurons during training.</li>
                    </ul>
                </ul>
            </article>

            <article class="summary">
                <h2>Summary</h2>
                <p>Training a neural network involves forward propagation, loss calculation, backpropagation, and
                    optimization. Understanding these steps is key to building efficient and accurate deep learning
                    models. In the next section, we will explore how to evaluate model performance.</p>
            </article>
        </section>
    </div>


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.0.11/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>

</html>