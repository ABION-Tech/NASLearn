<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Model Evaluation</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body {
            background-color: white;
            color: black;
            font-family: Arial, sans-serif;
        }

        .content-section {
            margin: 40px 20px;
            padding: 20px;
            border: 1px solid #e0e0e0;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        h1,
        h2,
        h3,
        h4 {
            color: #007bff;
            /* Blue accent */
        }

        s .table th,
        .table td {
            vertical-align: middle;
        }

        .highlight {
            background-color: #f0f8ff;
            border-left: 5px solid #007BFF;
            padding: 10px;
            margin: 20px 0;
        }

        footer {
            display: none;
        }

        @media (max-width: 768px) {
            .content-section {
                margin: 20px 10px;
                padding: 15px;
            }
        }
    </style>
</head>

<body>
    <div class="container mt-5">
        <div class="row">
            <div class="col-12">
                <h1 class="display-4 text-center">Topic 3: Model Evaluation</h1>
                <p class="lead text-center">Understanding How to Evaluate Machine Learning Models</p>
            </div>
        </div>

        <div class="row mt-4">
            <div class="col-12">
                <p>
                    Evaluating the performance of machine learning models is crucial to ensure they produce accurate,
                    reliable, and actionable results. This section covers key evaluation metrics and the confusion
                    matrix, which collectively offer insights into a model's strengths and weaknesses.
                </p>
            </div>
        </div>

        <div class="row mt-4">
            <div class="col-12">
                <h3>1. Evaluation Metrics</h3>
                <p>Evaluation metrics quantify a modelâ€™s predictive performance, particularly for classification tasks.
                    Below are the most commonly used metrics:</p>

                <div class="mb-4">
                    <h5>a. Accuracy</h5>
                    <p><strong>Definition:</strong> Proportion of correctly predicted instances out of the total number
                        of instances.</p>
                    <p><strong>Formula:</strong></p>
                    <pre><code>Accuracy = (Number of correct predictions) / (Total number of predictions)</code></pre>
                    <p><strong>Example Use Case:</strong> Useful for balanced datasets where the number of positive and
                        negative outcomes is roughly equal.</p>
                    <p><strong>Limitations:</strong> Can be misleading for imbalanced datasets (e.g., predicting rare
                        diseases).</p>
                </div>

                <div class="mb-4">
                    <h5>b. Precision</h5>
                    <p><strong>Definition:</strong> The proportion of positive predictions that are actually correct.
                        Precision emphasizes the quality of positive predictions.</p>
                    <p><strong>Formula:</strong></p>
                    <pre><code>Precision = True Positives (TP) / (True Positives (TP) + False Positives (FP))</code></pre>
                    <p><strong>Example Use Case:</strong> Important in applications like spam detection, where false
                        positives (non-spam flagged as spam) must be minimized.</p>
                </div>

                <div class="mb-4">
                    <h5>c. Recall (Sensitivity)</h5>
                    <p><strong>Definition:</strong> The proportion of actual positive instances correctly identified by
                        the model. Recall focuses on capturing all relevant instances.</p>
                    <p><strong>Formula:</strong></p>
                    <pre><code>Recall = True Positives (TP) / (True Positives (TP) + False Negatives (FN))</code></pre>
                    <p><strong>Example Use Case:</strong> Critical in medical diagnostics, where failing to identify a
                        positive case (e.g., a disease) can have severe consequences.</p>
                </div>

                <div class="mb-4">
                    <h5>d. F1 Score</h5>
                    <p><strong>Definition:</strong> The harmonic mean of precision and recall, offering a balanced
                        evaluation metric for imbalanced datasets.</p>
                    <p><strong>Formula:</strong></p>
                    <pre><code>F1 Score = 2 * (Precision * Recall) / (Precision + Recall)</code></pre>
                    <p><strong>Example Use Case:</strong> Effective when both false positives and false negatives are
                        costly, such as in fraud detection.</p>
                </div>
            </div>
        </div>

        <div class="row mt-4">
            <div class="col-12">
                <h3>2. Confusion Matrix</h3>
                <p>The confusion matrix is a powerful visualization tool that compares actual outcomes with predicted
                    outcomes. It provides a detailed breakdown of model performance.</p>

                <p><strong>Structure:</strong></p>
                <table class="table table-bordered">
                    <thead>
                        <tr>
                            <th></th>
                            <th>Predicted Positive</th>
                            <th>Predicted Negative</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Actual Positive</strong></td>
                            <td>True Positive (TP)</td>
                            <td>False Negative (FN)</td>
                        </tr>
                        <tr>
                            <td><strong>Actual Negative</strong></td>
                            <td>False Positive (FP)</td>
                            <td>True Negative (TN)</td>
                        </tr>
                    </tbody>
                </table>

                <p><strong>Key Insights from the Confusion Matrix:</strong></p>
                <ul>
                    <li><strong>True Positives (TP):</strong> Correctly identified positive instances.</li>
                    <li><strong>True Negatives (TN):</strong> Correctly identified negative instances.</li>
                    <li><strong>False Positives (FP):</strong> Incorrectly classified negative instances as positive.
                    </li>
                    <li><strong>False Negatives (FN):</strong> Missed positive instances classified as negative.</li>
                </ul>

                <p><strong>Example Use Case:</strong> In credit card fraud detection, the confusion matrix helps
                    identify how many fraudulent transactions were caught (TP), missed (FN), or incorrectly flagged
                    (FP).</p>
            </div>
        </div>

        <div class="row mt-5">
            <div class="col-12">
                <p>By combining evaluation metrics and the confusion matrix, machine learning practitioners can
                    comprehensively assess model performance, identify shortcomings, and refine algorithms to enhance
                    accuracy and reliability.</p>
            </div>
        </div>
    </div>


    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.6/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>

</html>